{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "001.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVipXAQFGGQrtChbQCyGnR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AtaruOhto/pytorch_learning/blob/master/001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBsfEctSmTZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "class SampleData:\n",
        "  \"\"\"\n",
        "    データ: X, y\n",
        "    オリジナルの重み: original_weight\n",
        "    を準備する。\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    # 元々の重み\n",
        "    original_weight = torch.Tensor([1, 2, 3])\n",
        "    # データ形式: torch.Size([3])\n",
        "\n",
        "    # Xのデータ準備 \n",
        "    # データ形式: torch.Size([100, 3])\n",
        "    X = torch.cat([torch.ones(100, 1), torch.randn(100, 2)], 1)\n",
        "\n",
        "    # データと重みの内積を計算する。\n",
        "    dot = torch.mv(X, original_weight)\n",
        "    \n",
        "    # 内積に乱数を足してyとする。\n",
        "    # データ形式: torch.Size([100])\n",
        "    y = dot + torch.randn(100) * 0.5    \n",
        "\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.original_weight = original_weight\n",
        "\n",
        "def exec_gradient_descent(weight, X, y):\n",
        "  # 学習率を0.1とする\n",
        "  eta = 0.1\n",
        "\n",
        "  # 損失関数のログ (下のループ中で誤差を格納していく)\n",
        "  losses = []\n",
        "\n",
        "  # 50回ループさせて、勾配降下法でパラメータを最適化する\n",
        "  for epoc in range(50):\n",
        "    # 前のループ中のbackward()メソッドで計算された勾配の値を削除\n",
        "    weight.grad = None\n",
        "    \n",
        "    # yの予測値を計算する\n",
        "    y_pred = torch.mv(X, weight)\n",
        "\n",
        "    # 平均二乗誤差Mean Square Error を計算 (実際の値と予測値のズレがどれだけあるかの指標のひとつ)\n",
        "    # 誤差が小さいほどモデルの性能が高い。\n",
        "    loss = torch.mean((y - y_pred) ** 2)\n",
        "\n",
        "    # 誤差逆伝播して、勾配を計算。\n",
        "    # weightのrequires_grad=Trueにしているので、自動で微分が計算される。\n",
        "    loss.backward()\n",
        "\n",
        "    # 重みを更新する。\n",
        "    weight.data = weight.data - eta * weight.grad.data\n",
        "\n",
        "    # ループの終わりにlossを格納する。誤差が小さくなっていくのを確認。\n",
        "    losses.append(loss.item())\n",
        "  return weight\n",
        "\n",
        "data = SampleData()\n",
        "\n",
        "# Tensorを乱数で初期化して、重みとする。\n",
        "# (以下でこの重みをループ中で、「実際のyと予測値y_predとの誤差を小さくするように」更新していく。)\n",
        "# requires_gradをTrueにすることで重みの自動微分を有効にする。\n",
        "# https://pytorch.org/docs/stable/notes/autograd.html#requires-grad\n",
        "weight_will_updated = torch.randn(3, requires_grad=True)\n",
        "\n",
        "# 乱数で作った重み (weight_will_updated) を更新する。\n",
        "# (Xの計算式から、yの値を正しく予測するように重みを更新する。)\n",
        "weight = exec_gradient_descent(weight_will_updated, data.X, data.y)\n",
        "\n",
        "# matplotlibで誤差 (loss) の推移を描画する。\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(losses)\n",
        "\n",
        "# 元々の重みと、更新された重みの差分を見る。\n",
        "# 差分は極小になっている。\n",
        "print(torch.abs(data.original_weight - weight))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}